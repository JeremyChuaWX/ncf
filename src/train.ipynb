{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "039a6dc5-3dca-4091-93db-a6659dffde1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n",
      "Range of userId is [0, 6039]\n",
      "Unique userIds 6040\n",
      "Range of itemId is [0, 3705]\n",
      "Unique itemIds 3706\n",
      "Range of rating is [1, 5]\n",
      "Unique ratings 5\n",
      "initialise sample generator\n",
      "start traing model\n",
      "GMF(\n",
      "  (embedding_user): Embedding(6040, 8)\n",
      "  (embedding_item): Embedding(3706, 8)\n",
      "  (affine_output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (logistic): Sigmoid()\n",
      ")\n",
      "Epoch 0 starts\n",
      "--------------------------------------------------------------------------------\n",
      "[Training Epoch 0] Batch 0, Loss 0.1780375987291336\n",
      "[Training Epoch 0] Batch 1, Loss 0.17347502708435059\n",
      "[Training Epoch 0] Batch 2, Loss 0.18350522220134735\n",
      "[Training Epoch 0] Batch 3, Loss 0.18533121049404144\n",
      "[Training Epoch 0] Batch 4, Loss 0.18095162510871887\n",
      "[Training Epoch 0] Batch 5, Loss 0.18043309450149536\n",
      "[Training Epoch 0] Batch 6, Loss 0.180410236120224\n",
      "[Training Epoch 0] Batch 7, Loss 0.17861086130142212\n",
      "[Training Epoch 0] Batch 8, Loss 0.1760123074054718\n",
      "[Training Epoch 0] Batch 9, Loss 0.17859286069869995\n",
      "[Training Epoch 0] Batch 10, Loss 0.17721880972385406\n",
      "[Training Epoch 0] Batch 11, Loss 0.17668381333351135\n",
      "[Training Epoch 0] Batch 12, Loss 0.17825841903686523\n",
      "[Training Epoch 0] Batch 13, Loss 0.1744571328163147\n",
      "[Training Epoch 0] Batch 14, Loss 0.1826179325580597\n",
      "[Training Epoch 0] Batch 15, Loss 0.17549890279769897\n",
      "[Training Epoch 0] Batch 16, Loss 0.17892120778560638\n",
      "[Training Epoch 0] Batch 17, Loss 0.172263041138649\n",
      "[Training Epoch 0] Batch 18, Loss 0.1820801943540573\n",
      "[Training Epoch 0] Batch 19, Loss 0.17211879789829254\n",
      "[Training Epoch 0] Batch 20, Loss 0.17775940895080566\n",
      "[Training Epoch 0] Batch 21, Loss 0.16940876841545105\n",
      "[Training Epoch 0] Batch 22, Loss 0.17746689915657043\n",
      "[Training Epoch 0] Batch 23, Loss 0.1704757958650589\n",
      "[Training Epoch 0] Batch 24, Loss 0.18051345646381378\n",
      "[Training Epoch 0] Batch 25, Loss 0.1769496500492096\n",
      "[Training Epoch 0] Batch 26, Loss 0.17434313893318176\n",
      "[Training Epoch 0] Batch 27, Loss 0.17343923449516296\n",
      "[Training Epoch 0] Batch 28, Loss 0.1747855246067047\n",
      "[Training Epoch 0] Batch 29, Loss 0.17600804567337036\n",
      "[Training Epoch 0] Batch 30, Loss 0.17556792497634888\n",
      "[Training Epoch 0] Batch 31, Loss 0.1720402091741562\n",
      "[Training Epoch 0] Batch 32, Loss 0.1765323281288147\n",
      "[Training Epoch 0] Batch 33, Loss 0.17319464683532715\n",
      "[Training Epoch 0] Batch 34, Loss 0.17185372114181519\n",
      "[Training Epoch 0] Batch 35, Loss 0.17094561457633972\n",
      "[Training Epoch 0] Batch 36, Loss 0.16854068636894226\n",
      "[Training Epoch 0] Batch 37, Loss 0.1708754152059555\n",
      "[Training Epoch 0] Batch 38, Loss 0.17496034502983093\n",
      "[Training Epoch 0] Batch 39, Loss 0.1689041256904602\n",
      "[Training Epoch 0] Batch 40, Loss 0.1739397495985031\n",
      "[Training Epoch 0] Batch 41, Loss 0.16737046837806702\n",
      "[Training Epoch 0] Batch 42, Loss 0.16695377230644226\n",
      "[Training Epoch 0] Batch 43, Loss 0.17314471304416656\n",
      "[Training Epoch 0] Batch 44, Loss 0.16620120406150818\n",
      "[Training Epoch 0] Batch 45, Loss 0.16749081015586853\n",
      "[Training Epoch 0] Batch 46, Loss 0.16656213998794556\n",
      "[Training Epoch 0] Batch 47, Loss 0.16538916528224945\n",
      "[Training Epoch 0] Batch 48, Loss 0.16920945048332214\n",
      "[Training Epoch 0] Batch 49, Loss 0.16721071302890778\n",
      "[Training Epoch 0] Batch 50, Loss 0.16771811246871948\n",
      "[Training Epoch 0] Batch 51, Loss 0.16820314526557922\n",
      "[Training Epoch 0] Batch 52, Loss 0.16559728980064392\n",
      "[Training Epoch 0] Batch 53, Loss 0.17004472017288208\n",
      "[Training Epoch 0] Batch 54, Loss 0.16708551347255707\n",
      "[Training Epoch 0] Batch 55, Loss 0.16798940300941467\n",
      "[Training Epoch 0] Batch 56, Loss 0.17100438475608826\n",
      "[Training Epoch 0] Batch 57, Loss 0.16463354229927063\n",
      "[Training Epoch 0] Batch 58, Loss 0.1641760766506195\n",
      "[Training Epoch 0] Batch 59, Loss 0.16574344038963318\n",
      "[Training Epoch 0] Batch 60, Loss 0.16393229365348816\n",
      "[Training Epoch 0] Batch 61, Loss 0.1643659621477127\n",
      "[Training Epoch 0] Batch 62, Loss 0.1652611792087555\n",
      "[Training Epoch 0] Batch 63, Loss 0.16629570722579956\n",
      "[Training Epoch 0] Batch 64, Loss 0.1631283164024353\n",
      "[Training Epoch 0] Batch 65, Loss 0.16204793751239777\n",
      "[Training Epoch 0] Batch 66, Loss 0.16631127893924713\n",
      "[Training Epoch 0] Batch 67, Loss 0.16448917984962463\n",
      "[Training Epoch 0] Batch 68, Loss 0.1674361675977707\n",
      "[Training Epoch 0] Batch 69, Loss 0.16163672506809235\n",
      "[Training Epoch 0] Batch 70, Loss 0.16666780412197113\n",
      "[Training Epoch 0] Batch 71, Loss 0.16765861213207245\n",
      "[Training Epoch 0] Batch 72, Loss 0.16141477227210999\n",
      "[Training Epoch 0] Batch 73, Loss 0.16085222363471985\n",
      "[Training Epoch 0] Batch 74, Loss 0.15860013663768768\n",
      "[Training Epoch 0] Batch 75, Loss 0.1658858358860016\n",
      "[Training Epoch 0] Batch 76, Loss 0.16403637826442719\n",
      "[Training Epoch 0] Batch 77, Loss 0.1620577722787857\n",
      "[Training Epoch 0] Batch 78, Loss 0.16489136219024658\n",
      "[Training Epoch 0] Batch 79, Loss 0.16718485951423645\n",
      "[Training Epoch 0] Batch 80, Loss 0.16505828499794006\n",
      "[Training Epoch 0] Batch 81, Loss 0.1602465808391571\n",
      "[Training Epoch 0] Batch 82, Loss 0.1641193926334381\n",
      "[Training Epoch 0] Batch 83, Loss 0.16275545954704285\n",
      "[Training Epoch 0] Batch 84, Loss 0.15841573476791382\n",
      "[Training Epoch 0] Batch 85, Loss 0.15907904505729675\n",
      "[Training Epoch 0] Batch 86, Loss 0.16178210079669952\n",
      "[Training Epoch 0] Batch 87, Loss 0.16449490189552307\n",
      "[Training Epoch 0] Batch 88, Loss 0.16025379300117493\n",
      "[Training Epoch 0] Batch 89, Loss 0.15759187936782837\n",
      "[Training Epoch 0] Batch 90, Loss 0.16263756155967712\n",
      "[Training Epoch 0] Batch 91, Loss 0.1579543948173523\n",
      "[Training Epoch 0] Batch 92, Loss 0.16232401132583618\n",
      "[Training Epoch 0] Batch 93, Loss 0.15843629837036133\n",
      "[Training Epoch 0] Batch 94, Loss 0.15875360369682312\n",
      "[Training Epoch 0] Batch 95, Loss 0.15537434816360474\n",
      "[Training Epoch 0] Batch 96, Loss 0.1587003767490387\n",
      "[Training Epoch 0] Batch 97, Loss 0.1572316735982895\n",
      "[Training Epoch 0] Batch 98, Loss 0.16209930181503296\n",
      "[Training Epoch 0] Batch 99, Loss 0.1638617217540741\n",
      "[Training Epoch 0] Batch 100, Loss 0.16256149113178253\n",
      "[Training Epoch 0] Batch 101, Loss 0.16031277179718018\n",
      "[Training Epoch 0] Batch 102, Loss 0.15889379382133484\n",
      "[Training Epoch 0] Batch 103, Loss 0.15575480461120605\n",
      "[Training Epoch 0] Batch 104, Loss 0.16097912192344666\n",
      "[Training Epoch 0] Batch 105, Loss 0.1650695502758026\n",
      "[Training Epoch 0] Batch 106, Loss 0.15638519823551178\n",
      "[Training Epoch 0] Batch 107, Loss 0.16026178002357483\n",
      "[Training Epoch 0] Batch 108, Loss 0.15388673543930054\n",
      "[Training Epoch 0] Batch 109, Loss 0.15824449062347412\n",
      "[Training Epoch 0] Batch 110, Loss 0.15475866198539734\n",
      "[Training Epoch 0] Batch 111, Loss 0.15709072351455688\n",
      "[Training Epoch 0] Batch 112, Loss 0.1529313623905182\n",
      "[Training Epoch 0] Batch 113, Loss 0.15835173428058624\n",
      "[Training Epoch 0] Batch 114, Loss 0.1600780487060547\n",
      "[Training Epoch 0] Batch 115, Loss 0.15853948891162872\n",
      "[Training Epoch 0] Batch 116, Loss 0.15717840194702148\n",
      "[Training Epoch 0] Batch 117, Loss 0.1589927077293396\n",
      "[Training Epoch 0] Batch 118, Loss 0.15482008457183838\n",
      "[Training Epoch 0] Batch 119, Loss 0.15447194874286652\n",
      "[Training Epoch 0] Batch 120, Loss 0.1550174057483673\n",
      "[Training Epoch 0] Batch 121, Loss 0.15457384288311005\n",
      "[Training Epoch 0] Batch 122, Loss 0.15244880318641663\n",
      "[Training Epoch 0] Batch 123, Loss 0.15319722890853882\n",
      "[Training Epoch 0] Batch 124, Loss 0.15656346082687378\n",
      "[Training Epoch 0] Batch 125, Loss 0.1552988588809967\n",
      "[Training Epoch 0] Batch 126, Loss 0.153438538312912\n",
      "[Training Epoch 0] Batch 127, Loss 0.15590044856071472\n",
      "[Training Epoch 0] Batch 128, Loss 0.1531916856765747\n",
      "[Training Epoch 0] Batch 129, Loss 0.15661263465881348\n",
      "[Training Epoch 0] Batch 130, Loss 0.1549934297800064\n",
      "[Training Epoch 0] Batch 131, Loss 0.15519832074642181\n",
      "[Training Epoch 0] Batch 132, Loss 0.15222536027431488\n",
      "[Training Epoch 0] Batch 133, Loss 0.15473833680152893\n",
      "[Training Epoch 0] Batch 134, Loss 0.1528572291135788\n",
      "[Training Epoch 0] Batch 135, Loss 0.15245752036571503\n",
      "[Training Epoch 0] Batch 136, Loss 0.1537805199623108\n",
      "[Training Epoch 0] Batch 137, Loss 0.15477997064590454\n",
      "[Training Epoch 0] Batch 138, Loss 0.1538790762424469\n",
      "[Training Epoch 0] Batch 139, Loss 0.15357688069343567\n",
      "[Training Epoch 0] Batch 140, Loss 0.14972549676895142\n",
      "[Training Epoch 0] Batch 141, Loss 0.1513618528842926\n",
      "[Training Epoch 0] Batch 142, Loss 0.15363729000091553\n",
      "[Training Epoch 0] Batch 143, Loss 0.15180161595344543\n",
      "[Training Epoch 0] Batch 144, Loss 0.15326109528541565\n",
      "[Training Epoch 0] Batch 145, Loss 0.15139472484588623\n",
      "[Training Epoch 0] Batch 146, Loss 0.15490731596946716\n",
      "[Training Epoch 0] Batch 147, Loss 0.15269333124160767\n",
      "[Training Epoch 0] Batch 148, Loss 0.14824067056179047\n",
      "[Training Epoch 0] Batch 149, Loss 0.15219944715499878\n",
      "[Training Epoch 0] Batch 150, Loss 0.1491202563047409\n",
      "[Training Epoch 0] Batch 151, Loss 0.15092334151268005\n",
      "[Training Epoch 0] Batch 152, Loss 0.1550174355506897\n",
      "[Training Epoch 0] Batch 153, Loss 0.15246812999248505\n",
      "[Training Epoch 0] Batch 154, Loss 0.14871743321418762\n",
      "[Training Epoch 0] Batch 155, Loss 0.15098318457603455\n",
      "[Training Epoch 0] Batch 156, Loss 0.14696983993053436\n",
      "[Training Epoch 0] Batch 157, Loss 0.14956527948379517\n",
      "[Training Epoch 0] Batch 158, Loss 0.15200647711753845\n",
      "[Training Epoch 0] Batch 159, Loss 0.1533140242099762\n",
      "[Training Epoch 0] Batch 160, Loss 0.15004631876945496\n",
      "[Training Epoch 0] Batch 161, Loss 0.15249821543693542\n",
      "[Training Epoch 0] Batch 162, Loss 0.14946240186691284\n",
      "[Training Epoch 0] Batch 163, Loss 0.14967364072799683\n",
      "[Training Epoch 0] Batch 164, Loss 0.15197351574897766\n",
      "[Training Epoch 0] Batch 165, Loss 0.15260648727416992\n",
      "[Training Epoch 0] Batch 166, Loss 0.1551394760608673\n",
      "[Training Epoch 0] Batch 167, Loss 0.15151843428611755\n",
      "[Training Epoch 0] Batch 168, Loss 0.15079715847969055\n",
      "[Training Epoch 0] Batch 169, Loss 0.14987929165363312\n",
      "[Training Epoch 0] Batch 170, Loss 0.14781774580478668\n",
      "[Training Epoch 0] Batch 171, Loss 0.14760959148406982\n",
      "[Training Epoch 0] Batch 172, Loss 0.1461954414844513\n",
      "[Training Epoch 0] Batch 173, Loss 0.1501244604587555\n",
      "[Training Epoch 0] Batch 174, Loss 0.15058842301368713\n",
      "[Training Epoch 0] Batch 175, Loss 0.14853933453559875\n",
      "[Training Epoch 0] Batch 176, Loss 0.14660440385341644\n",
      "[Training Epoch 0] Batch 177, Loss 0.14639367163181305\n",
      "[Training Epoch 0] Batch 178, Loss 0.14825281500816345\n",
      "[Training Epoch 0] Batch 179, Loss 0.14775589108467102\n",
      "[Training Epoch 0] Batch 180, Loss 0.1487932801246643\n",
      "[Training Epoch 0] Batch 181, Loss 0.14638537168502808\n",
      "[Training Epoch 0] Batch 182, Loss 0.15004098415374756\n",
      "[Training Epoch 0] Batch 183, Loss 0.14651621878147125\n",
      "[Training Epoch 0] Batch 184, Loss 0.14769704639911652\n",
      "[Training Epoch 0] Batch 185, Loss 0.14686334133148193\n",
      "[Training Epoch 0] Batch 186, Loss 0.14312893152236938\n",
      "[Training Epoch 0] Batch 187, Loss 0.14464735984802246\n",
      "[Training Epoch 0] Batch 188, Loss 0.1472633183002472\n",
      "[Training Epoch 0] Batch 189, Loss 0.14270123839378357\n",
      "[Training Epoch 0] Batch 190, Loss 0.14628036320209503\n",
      "[Training Epoch 0] Batch 191, Loss 0.14397820830345154\n",
      "[Training Epoch 0] Batch 192, Loss 0.1445809006690979\n",
      "[Training Epoch 0] Batch 193, Loss 0.1500203162431717\n",
      "[Training Epoch 0] Batch 194, Loss 0.1474737673997879\n",
      "[Training Epoch 0] Batch 195, Loss 0.14931097626686096\n",
      "[Training Epoch 0] Batch 196, Loss 0.14708897471427917\n",
      "[Training Epoch 0] Batch 197, Loss 0.14579737186431885\n",
      "[Training Epoch 0] Batch 198, Loss 0.14361503720283508\n",
      "[Training Epoch 0] Batch 199, Loss 0.1466001719236374\n",
      "[Training Epoch 0] Batch 200, Loss 0.14709612727165222\n",
      "[Training Epoch 0] Batch 201, Loss 0.14561855792999268\n",
      "[Training Epoch 0] Batch 202, Loss 0.14418649673461914\n",
      "[Training Epoch 0] Batch 203, Loss 0.144407719373703\n",
      "[Training Epoch 0] Batch 204, Loss 0.14508888125419617\n",
      "[Training Epoch 0] Batch 205, Loss 0.14684420824050903\n",
      "[Training Epoch 0] Batch 206, Loss 0.14532232284545898\n",
      "[Training Epoch 0] Batch 207, Loss 0.14533191919326782\n",
      "[Training Epoch 0] Batch 208, Loss 0.14629830420017242\n",
      "[Training Epoch 0] Batch 209, Loss 0.1443038433790207\n",
      "[Training Epoch 0] Batch 210, Loss 0.14429326355457306\n",
      "[Training Epoch 0] Batch 211, Loss 0.1417858749628067\n",
      "[Training Epoch 0] Batch 212, Loss 0.14403505623340607\n",
      "[Training Epoch 0] Batch 213, Loss 0.14524173736572266\n",
      "[Training Epoch 0] Batch 214, Loss 0.1423506736755371\n",
      "[Training Epoch 0] Batch 215, Loss 0.1452047973871231\n",
      "[Training Epoch 0] Batch 216, Loss 0.1429528295993805\n",
      "[Training Epoch 0] Batch 217, Loss 0.1426408737897873\n",
      "[Training Epoch 0] Batch 218, Loss 0.1419292688369751\n",
      "[Training Epoch 0] Batch 219, Loss 0.14276471734046936\n",
      "[Training Epoch 0] Batch 220, Loss 0.14656075835227966\n",
      "[Training Epoch 0] Batch 221, Loss 0.14091932773590088\n",
      "[Training Epoch 0] Batch 222, Loss 0.14439597725868225\n",
      "[Training Epoch 0] Batch 223, Loss 0.14446157217025757\n",
      "[Training Epoch 0] Batch 224, Loss 0.14216214418411255\n",
      "[Training Epoch 0] Batch 225, Loss 0.14023901522159576\n",
      "[Training Epoch 0] Batch 226, Loss 0.14187680184841156\n",
      "[Training Epoch 0] Batch 227, Loss 0.14437127113342285\n",
      "[Training Epoch 0] Batch 228, Loss 0.14301463961601257\n",
      "[Training Epoch 0] Batch 229, Loss 0.14286202192306519\n",
      "[Training Epoch 0] Batch 230, Loss 0.14065225422382355\n",
      "[Training Epoch 0] Batch 231, Loss 0.1390446424484253\n",
      "[Training Epoch 0] Batch 232, Loss 0.14175474643707275\n",
      "[Training Epoch 0] Batch 233, Loss 0.14333878457546234\n",
      "[Training Epoch 0] Batch 234, Loss 0.1415734887123108\n",
      "[Training Epoch 0] Batch 235, Loss 0.1405046135187149\n",
      "[Training Epoch 0] Batch 236, Loss 0.1430647075176239\n",
      "[Training Epoch 0] Batch 237, Loss 0.14159059524536133\n",
      "[Training Epoch 0] Batch 238, Loss 0.14045017957687378\n",
      "[Training Epoch 0] Batch 239, Loss 0.14067423343658447\n",
      "[Training Epoch 0] Batch 240, Loss 0.14023539423942566\n",
      "[Training Epoch 0] Batch 241, Loss 0.14104734361171722\n",
      "[Training Epoch 0] Batch 242, Loss 0.13922351598739624\n",
      "[Training Epoch 0] Batch 243, Loss 0.1405365765094757\n",
      "[Training Epoch 0] Batch 244, Loss 0.13950484991073608\n",
      "[Training Epoch 0] Batch 245, Loss 0.14043255150318146\n",
      "[Training Epoch 0] Batch 246, Loss 0.13980145752429962\n",
      "[Training Epoch 0] Batch 247, Loss 0.14122281968593597\n",
      "[Training Epoch 0] Batch 248, Loss 0.14001497626304626\n",
      "[Training Epoch 0] Batch 249, Loss 0.13845081627368927\n",
      "[Training Epoch 0] Batch 250, Loss 0.14102418720722198\n",
      "[Training Epoch 0] Batch 251, Loss 0.142341747879982\n",
      "[Training Epoch 0] Batch 252, Loss 0.14256247878074646\n",
      "[Training Epoch 0] Batch 253, Loss 0.13823817670345306\n",
      "[Training Epoch 0] Batch 254, Loss 0.14215166866779327\n",
      "[Training Epoch 0] Batch 255, Loss 0.13636618852615356\n",
      "[Training Epoch 0] Batch 256, Loss 0.13908489048480988\n",
      "[Training Epoch 0] Batch 257, Loss 0.14256969094276428\n",
      "[Training Epoch 0] Batch 258, Loss 0.1414610743522644\n",
      "[Training Epoch 0] Batch 259, Loss 0.14000536501407623\n",
      "[Training Epoch 0] Batch 260, Loss 0.1377307027578354\n",
      "[Training Epoch 0] Batch 261, Loss 0.1372266411781311\n",
      "[Training Epoch 0] Batch 262, Loss 0.13651829957962036\n",
      "[Training Epoch 0] Batch 263, Loss 0.13984841108322144\n",
      "[Training Epoch 0] Batch 264, Loss 0.13923244178295135\n",
      "[Training Epoch 0] Batch 265, Loss 0.14146657288074493\n",
      "[Training Epoch 0] Batch 266, Loss 0.13877084851264954\n",
      "[Training Epoch 0] Batch 267, Loss 0.1384163200855255\n",
      "[Training Epoch 0] Batch 268, Loss 0.14155854284763336\n",
      "[Training Epoch 0] Batch 269, Loss 0.14041614532470703\n",
      "[Training Epoch 0] Batch 270, Loss 0.13900434970855713\n",
      "[Training Epoch 0] Batch 271, Loss 0.13650977611541748\n",
      "[Training Epoch 0] Batch 272, Loss 0.13589128851890564\n",
      "[Training Epoch 0] Batch 273, Loss 0.1395016610622406\n",
      "[Training Epoch 0] Batch 274, Loss 0.1425669938325882\n",
      "[Training Epoch 0] Batch 275, Loss 0.14052748680114746\n",
      "[Training Epoch 0] Batch 276, Loss 0.1412610411643982\n",
      "[Training Epoch 0] Batch 277, Loss 0.13740363717079163\n",
      "[Training Epoch 0] Batch 278, Loss 0.13558553159236908\n",
      "[Training Epoch 0] Batch 279, Loss 0.1374448984861374\n",
      "[Training Epoch 0] Batch 280, Loss 0.14216768741607666\n",
      "[Training Epoch 0] Batch 281, Loss 0.13649454712867737\n",
      "[Training Epoch 0] Batch 282, Loss 0.13671833276748657\n",
      "[Training Epoch 0] Batch 283, Loss 0.14037349820137024\n",
      "[Training Epoch 0] Batch 284, Loss 0.13789160549640656\n",
      "[Training Epoch 0] Batch 285, Loss 0.13681364059448242\n",
      "[Training Epoch 0] Batch 286, Loss 0.13463015854358673\n",
      "[Training Epoch 0] Batch 287, Loss 0.13486309349536896\n",
      "[Training Epoch 0] Batch 288, Loss 0.1377859264612198\n",
      "[Training Epoch 0] Batch 289, Loss 0.1371038258075714\n",
      "[Training Epoch 0] Batch 290, Loss 0.13518314063549042\n",
      "[Training Epoch 0] Batch 291, Loss 0.13438624143600464\n",
      "[Training Epoch 0] Batch 292, Loss 0.137762650847435\n",
      "[Training Epoch 0] Batch 293, Loss 0.1362992525100708\n",
      "[Training Epoch 0] Batch 294, Loss 0.1364556849002838\n",
      "[Training Epoch 0] Batch 295, Loss 0.1387295424938202\n",
      "[Training Epoch 0] Batch 296, Loss 0.13489431142807007\n",
      "[Training Epoch 0] Batch 297, Loss 0.13355638086795807\n",
      "[Training Epoch 0] Batch 298, Loss 0.1342301219701767\n",
      "[Training Epoch 0] Batch 299, Loss 0.13455830514431\n",
      "[Training Epoch 0] Batch 300, Loss 0.13711410760879517\n",
      "[Training Epoch 0] Batch 301, Loss 0.1366242617368698\n",
      "[Training Epoch 0] Batch 302, Loss 0.13444191217422485\n",
      "[Training Epoch 0] Batch 303, Loss 0.13459575176239014\n",
      "[Training Epoch 0] Batch 304, Loss 0.13749223947525024\n",
      "[Training Epoch 0] Batch 305, Loss 0.13476862013339996\n",
      "[Training Epoch 0] Batch 306, Loss 0.13404390215873718\n",
      "[Training Epoch 0] Batch 307, Loss 0.13226674497127533\n",
      "[Training Epoch 0] Batch 308, Loss 0.1334059238433838\n",
      "[Training Epoch 0] Batch 309, Loss 0.13456782698631287\n",
      "[Training Epoch 0] Batch 310, Loss 0.13780885934829712\n",
      "[Training Epoch 0] Batch 311, Loss 0.13777035474777222\n",
      "[Training Epoch 0] Batch 312, Loss 0.13535244762897491\n",
      "[Training Epoch 0] Batch 313, Loss 0.13965383172035217\n",
      "[Training Epoch 0] Batch 314, Loss 0.13558092713356018\n",
      "[Training Epoch 0] Batch 315, Loss 0.13297444581985474\n",
      "[Training Epoch 0] Batch 316, Loss 0.13255348801612854\n",
      "[Training Epoch 0] Batch 317, Loss 0.1308281421661377\n",
      "[Training Epoch 0] Batch 318, Loss 0.13356506824493408\n",
      "[Training Epoch 0] Batch 319, Loss 0.13500121235847473\n",
      "[Training Epoch 0] Batch 320, Loss 0.13270723819732666\n",
      "[Training Epoch 0] Batch 321, Loss 0.13235963881015778\n",
      "[Training Epoch 0] Batch 322, Loss 0.13293585181236267\n",
      "[Training Epoch 0] Batch 323, Loss 0.13519203662872314\n",
      "[Training Epoch 0] Batch 324, Loss 0.13416221737861633\n",
      "[Training Epoch 0] Batch 325, Loss 0.1335887610912323\n",
      "[Training Epoch 0] Batch 326, Loss 0.13461516797542572\n",
      "[Training Epoch 0] Batch 327, Loss 0.13638406991958618\n",
      "[Training Epoch 0] Batch 328, Loss 0.13613015413284302\n",
      "[Training Epoch 0] Batch 329, Loss 0.1318463683128357\n",
      "[Training Epoch 0] Batch 330, Loss 0.13393960893154144\n",
      "[Training Epoch 0] Batch 331, Loss 0.13809889554977417\n",
      "[Training Epoch 0] Batch 332, Loss 0.1364225149154663\n",
      "[Training Epoch 0] Batch 333, Loss 0.1321895718574524\n",
      "[Training Epoch 0] Batch 334, Loss 0.1357196718454361\n",
      "[Training Epoch 0] Batch 335, Loss 0.1318809688091278\n",
      "[Training Epoch 0] Batch 336, Loss 0.13479265570640564\n",
      "[Training Epoch 0] Batch 337, Loss 0.1298876404762268\n",
      "[Training Epoch 0] Batch 338, Loss 0.1336050033569336\n",
      "[Training Epoch 0] Batch 339, Loss 0.1349073350429535\n",
      "[Training Epoch 0] Batch 340, Loss 0.134438157081604\n",
      "[Training Epoch 0] Batch 341, Loss 0.1319054663181305\n",
      "[Training Epoch 0] Batch 342, Loss 0.1316477656364441\n",
      "[Training Epoch 0] Batch 343, Loss 0.13316603004932404\n",
      "[Training Epoch 0] Batch 344, Loss 0.13139665126800537\n",
      "[Training Epoch 0] Batch 345, Loss 0.13059359788894653\n",
      "[Training Epoch 0] Batch 346, Loss 0.13102254271507263\n",
      "[Training Epoch 0] Batch 347, Loss 0.12807774543762207\n",
      "[Training Epoch 0] Batch 348, Loss 0.13108336925506592\n",
      "[Training Epoch 0] Batch 349, Loss 0.12965789437294006\n",
      "[Training Epoch 0] Batch 350, Loss 0.12945322692394257\n",
      "[Training Epoch 0] Batch 351, Loss 0.13215602934360504\n",
      "[Training Epoch 0] Batch 352, Loss 0.1305767297744751\n",
      "[Training Epoch 0] Batch 353, Loss 0.12720297276973724\n",
      "[Training Epoch 0] Batch 354, Loss 0.12923753261566162\n",
      "[Training Epoch 0] Batch 355, Loss 0.13262434303760529\n",
      "[Training Epoch 0] Batch 356, Loss 0.13379833102226257\n",
      "[Training Epoch 0] Batch 357, Loss 0.1280265599489212\n",
      "[Training Epoch 0] Batch 358, Loss 0.13124732673168182\n",
      "[Training Epoch 0] Batch 359, Loss 0.13182811439037323\n",
      "[Training Epoch 0] Batch 360, Loss 0.13135141134262085\n",
      "[Training Epoch 0] Batch 361, Loss 0.1331571638584137\n",
      "[Training Epoch 0] Batch 362, Loss 0.13227441906929016\n",
      "[Training Epoch 0] Batch 363, Loss 0.12874993681907654\n",
      "[Training Epoch 0] Batch 364, Loss 0.13261613249778748\n",
      "[Training Epoch 0] Batch 365, Loss 0.12596121430397034\n",
      "[Training Epoch 0] Batch 366, Loss 0.1291588842868805\n",
      "[Training Epoch 0] Batch 367, Loss 0.1298617571592331\n",
      "[Training Epoch 0] Batch 368, Loss 0.12643161416053772\n",
      "[Training Epoch 0] Batch 369, Loss 0.13153815269470215\n",
      "[Training Epoch 0] Batch 370, Loss 0.13184542953968048\n",
      "[Training Epoch 0] Batch 371, Loss 0.1342337280511856\n",
      "[Training Epoch 0] Batch 372, Loss 0.1294281780719757\n",
      "[Training Epoch 0] Batch 373, Loss 0.13593417406082153\n",
      "[Training Epoch 0] Batch 374, Loss 0.13284221291542053\n",
      "[Training Epoch 0] Batch 375, Loss 0.12807635962963104\n",
      "[Training Epoch 0] Batch 376, Loss 0.13338512182235718\n",
      "[Training Epoch 0] Batch 377, Loss 0.1294371485710144\n",
      "[Training Epoch 0] Batch 378, Loss 0.12929899990558624\n",
      "[Training Epoch 0] Batch 379, Loss 0.12880758941173553\n",
      "[Training Epoch 0] Batch 380, Loss 0.1288604438304901\n",
      "[Training Epoch 0] Batch 381, Loss 0.1273091733455658\n",
      "[Training Epoch 0] Batch 382, Loss 0.13035568594932556\n",
      "[Training Epoch 0] Batch 383, Loss 0.13014380633831024\n",
      "[Training Epoch 0] Batch 384, Loss 0.1292138546705246\n",
      "[Training Epoch 0] Batch 385, Loss 0.12918996810913086\n",
      "[Training Epoch 0] Batch 386, Loss 0.12800541520118713\n",
      "[Training Epoch 0] Batch 387, Loss 0.12924078106880188\n",
      "[Training Epoch 0] Batch 388, Loss 0.1304587423801422\n",
      "[Training Epoch 0] Batch 389, Loss 0.13052934408187866\n",
      "[Training Epoch 0] Batch 390, Loss 0.12839344143867493\n",
      "[Training Epoch 0] Batch 391, Loss 0.12860949337482452\n",
      "[Training Epoch 0] Batch 392, Loss 0.12915080785751343\n",
      "[Training Epoch 0] Batch 393, Loss 0.12900322675704956\n",
      "[Training Epoch 0] Batch 394, Loss 0.12993812561035156\n",
      "[Training Epoch 0] Batch 395, Loss 0.1291128695011139\n",
      "[Training Epoch 0] Batch 396, Loss 0.13032928109169006\n",
      "[Training Epoch 0] Batch 397, Loss 0.12772704660892487\n",
      "[Training Epoch 0] Batch 398, Loss 0.12863227725028992\n",
      "[Training Epoch 0] Batch 399, Loss 0.1284741908311844\n",
      "[Training Epoch 0] Batch 400, Loss 0.1260439157485962\n",
      "[Training Epoch 0] Batch 401, Loss 0.12724639475345612\n",
      "[Training Epoch 0] Batch 402, Loss 0.13034482300281525\n",
      "[Training Epoch 0] Batch 403, Loss 0.12472676485776901\n",
      "[Training Epoch 0] Batch 404, Loss 0.1266430914402008\n",
      "[Training Epoch 0] Batch 405, Loss 0.1283225417137146\n",
      "[Training Epoch 0] Batch 406, Loss 0.12647566199302673\n",
      "[Training Epoch 0] Batch 407, Loss 0.127740740776062\n",
      "[Training Epoch 0] Batch 408, Loss 0.12511926889419556\n",
      "[Training Epoch 0] Batch 409, Loss 0.13005805015563965\n",
      "[Training Epoch 0] Batch 410, Loss 0.12776236236095428\n",
      "[Training Epoch 0] Batch 411, Loss 0.12216976284980774\n",
      "[Training Epoch 0] Batch 412, Loss 0.12277098000049591\n",
      "[Training Epoch 0] Batch 413, Loss 0.12819553911685944\n",
      "[Training Epoch 0] Batch 414, Loss 0.12867337465286255\n",
      "[Training Epoch 0] Batch 415, Loss 0.12435846775770187\n",
      "[Training Epoch 0] Batch 416, Loss 0.1286303997039795\n",
      "[Training Epoch 0] Batch 417, Loss 0.12636162340641022\n",
      "[Training Epoch 0] Batch 418, Loss 0.1252005398273468\n",
      "[Training Epoch 0] Batch 419, Loss 0.12470792233943939\n",
      "[Training Epoch 0] Batch 420, Loss 0.12988151609897614\n",
      "[Training Epoch 0] Batch 421, Loss 0.12712973356246948\n",
      "[Training Epoch 0] Batch 422, Loss 0.13074073195457458\n",
      "[Training Epoch 0] Batch 423, Loss 0.12489590793848038\n",
      "[Training Epoch 0] Batch 424, Loss 0.1275399625301361\n",
      "[Training Epoch 0] Batch 425, Loss 0.12772631645202637\n",
      "[Training Epoch 0] Batch 426, Loss 0.12244816869497299\n",
      "[Training Epoch 0] Batch 427, Loss 0.12864592671394348\n",
      "[Training Epoch 0] Batch 428, Loss 0.1263047456741333\n",
      "[Training Epoch 0] Batch 429, Loss 0.12809820473194122\n",
      "[Training Epoch 0] Batch 430, Loss 0.1271025389432907\n",
      "[Training Epoch 0] Batch 431, Loss 0.12674985826015472\n",
      "[Training Epoch 0] Batch 432, Loss 0.12643608450889587\n",
      "[Training Epoch 0] Batch 433, Loss 0.12770241498947144\n",
      "[Training Epoch 0] Batch 434, Loss 0.12285187840461731\n",
      "[Training Epoch 0] Batch 435, Loss 0.12941771745681763\n",
      "[Training Epoch 0] Batch 436, Loss 0.1299600899219513\n",
      "[Training Epoch 0] Batch 437, Loss 0.12814921140670776\n",
      "[Training Epoch 0] Batch 438, Loss 0.1243465319275856\n",
      "[Training Epoch 0] Batch 439, Loss 0.13132822513580322\n",
      "[Training Epoch 0] Batch 440, Loss 0.12993650138378143\n",
      "[Training Epoch 0] Batch 441, Loss 0.12159892916679382\n",
      "[Training Epoch 0] Batch 442, Loss 0.12058235704898834\n",
      "[Training Epoch 0] Batch 443, Loss 0.12684567272663116\n",
      "[Training Epoch 0] Batch 444, Loss 0.12549731135368347\n",
      "[Training Epoch 0] Batch 445, Loss 0.12024801969528198\n",
      "[Training Epoch 0] Batch 446, Loss 0.1252116858959198\n",
      "[Training Epoch 0] Batch 447, Loss 0.12843039631843567\n",
      "[Training Epoch 0] Batch 448, Loss 0.12494148313999176\n",
      "[Training Epoch 0] Batch 449, Loss 0.12692712247371674\n",
      "[Training Epoch 0] Batch 450, Loss 0.12370392680168152\n",
      "[Training Epoch 0] Batch 451, Loss 0.12520554661750793\n",
      "[Training Epoch 0] Batch 452, Loss 0.12529730796813965\n",
      "[Training Epoch 0] Batch 453, Loss 0.12597709894180298\n",
      "[Training Epoch 0] Batch 454, Loss 0.12318891286849976\n",
      "[Training Epoch 0] Batch 455, Loss 0.12309867143630981\n",
      "[Training Epoch 0] Batch 456, Loss 0.12767142057418823\n",
      "[Training Epoch 0] Batch 457, Loss 0.12499627470970154\n",
      "[Training Epoch 0] Batch 458, Loss 0.1276101917028427\n",
      "[Training Epoch 0] Batch 459, Loss 0.12738776206970215\n",
      "[Training Epoch 0] Batch 460, Loss 0.12064681947231293\n",
      "[Training Epoch 0] Batch 461, Loss 0.122526615858078\n",
      "[Training Epoch 0] Batch 462, Loss 0.127623051404953\n",
      "[Training Epoch 0] Batch 463, Loss 0.12348075211048126\n",
      "[Training Epoch 0] Batch 464, Loss 0.1270119547843933\n",
      "[Training Epoch 0] Batch 465, Loss 0.12186338752508163\n",
      "[Training Epoch 0] Batch 466, Loss 0.1215958297252655\n",
      "[Training Epoch 0] Batch 467, Loss 0.12251056730747223\n",
      "[Training Epoch 0] Batch 468, Loss 0.12680581212043762\n",
      "[Training Epoch 0] Batch 469, Loss 0.11793084442615509\n",
      "[Training Epoch 0] Batch 470, Loss 0.11916705965995789\n",
      "[Training Epoch 0] Batch 471, Loss 0.12311355769634247\n",
      "[Training Epoch 0] Batch 472, Loss 0.12608522176742554\n",
      "[Training Epoch 0] Batch 473, Loss 0.12132993340492249\n",
      "[Training Epoch 0] Batch 474, Loss 0.12573513388633728\n",
      "[Training Epoch 0] Batch 475, Loss 0.12169348448514938\n",
      "[Training Epoch 0] Batch 476, Loss 0.1223912239074707\n",
      "[Training Epoch 0] Batch 477, Loss 0.12387870997190475\n",
      "[Training Epoch 0] Batch 478, Loss 0.12342894077301025\n",
      "[Training Epoch 0] Batch 479, Loss 0.12522074580192566\n",
      "[Training Epoch 0] Batch 480, Loss 0.12320318818092346\n",
      "[Training Epoch 0] Batch 481, Loss 0.12325255572795868\n",
      "[Training Epoch 0] Batch 482, Loss 0.12188460677862167\n",
      "[Training Epoch 0] Batch 483, Loss 0.12411828339099884\n",
      "[Training Epoch 0] Batch 484, Loss 0.1212691068649292\n",
      "[Training Epoch 0] Batch 485, Loss 0.117320716381073\n",
      "[Training Epoch 0] Batch 486, Loss 0.12139745056629181\n",
      "[Training Epoch 0] Batch 487, Loss 0.11933353543281555\n",
      "[Training Epoch 0] Batch 488, Loss 0.11870349943637848\n",
      "[Training Epoch 0] Batch 489, Loss 0.12193436175584793\n",
      "[Training Epoch 0] Batch 490, Loss 0.12065444886684418\n",
      "[Training Epoch 0] Batch 491, Loss 0.12534205615520477\n",
      "[Training Epoch 0] Batch 492, Loss 0.12667647004127502\n",
      "[Training Epoch 0] Batch 493, Loss 0.12023080140352249\n",
      "[Training Epoch 0] Batch 494, Loss 0.12354082614183426\n",
      "[Training Epoch 0] Batch 495, Loss 0.11848187446594238\n",
      "[Training Epoch 0] Batch 496, Loss 0.12284623831510544\n",
      "[Training Epoch 0] Batch 497, Loss 0.11904039978981018\n",
      "[Training Epoch 0] Batch 498, Loss 0.12080992758274078\n",
      "[Training Epoch 0] Batch 499, Loss 0.1187610924243927\n",
      "[Training Epoch 0] Batch 500, Loss 0.12217271327972412\n",
      "[Training Epoch 0] Batch 501, Loss 0.12270566821098328\n",
      "[Training Epoch 0] Batch 502, Loss 0.12138402462005615\n",
      "[Training Epoch 0] Batch 503, Loss 0.12418258190155029\n",
      "[Training Epoch 0] Batch 504, Loss 0.11762925982475281\n",
      "[Training Epoch 0] Batch 505, Loss 0.12002008408308029\n",
      "[Training Epoch 0] Batch 506, Loss 0.12499173730611801\n",
      "[Training Epoch 0] Batch 507, Loss 0.12025762349367142\n",
      "[Training Epoch 0] Batch 508, Loss 0.12302437424659729\n",
      "[Training Epoch 0] Batch 509, Loss 0.12534700334072113\n",
      "[Training Epoch 0] Batch 510, Loss 0.11732685565948486\n",
      "[Training Epoch 0] Batch 511, Loss 0.11948935687541962\n",
      "[Training Epoch 0] Batch 512, Loss 0.11959268897771835\n",
      "[Training Epoch 0] Batch 513, Loss 0.12111527472734451\n",
      "[Training Epoch 0] Batch 514, Loss 0.12527693808078766\n",
      "[Training Epoch 0] Batch 515, Loss 0.11782565712928772\n",
      "[Training Epoch 0] Batch 516, Loss 0.12071257084608078\n",
      "[Training Epoch 0] Batch 517, Loss 0.123967245221138\n",
      "[Training Epoch 0] Batch 518, Loss 0.12125008553266525\n",
      "[Training Epoch 0] Batch 519, Loss 0.11972110718488693\n",
      "[Training Epoch 0] Batch 520, Loss 0.11772271990776062\n",
      "[Training Epoch 0] Batch 521, Loss 0.12219012528657913\n",
      "[Training Epoch 0] Batch 522, Loss 0.12134399265050888\n",
      "[Training Epoch 0] Batch 523, Loss 0.12289075553417206\n",
      "[Training Epoch 0] Batch 524, Loss 0.11763264983892441\n",
      "[Training Epoch 0] Batch 525, Loss 0.12460673600435257\n",
      "[Training Epoch 0] Batch 526, Loss 0.12267458438873291\n",
      "[Training Epoch 0] Batch 527, Loss 0.12028500437736511\n",
      "[Training Epoch 0] Batch 528, Loss 0.12320759892463684\n",
      "[Training Epoch 0] Batch 529, Loss 0.12682101130485535\n",
      "[Training Epoch 0] Batch 530, Loss 0.12114356458187103\n",
      "[Training Epoch 0] Batch 531, Loss 0.12132004648447037\n",
      "[Training Epoch 0] Batch 532, Loss 0.12015838921070099\n",
      "[Training Epoch 0] Batch 533, Loss 0.12085162103176117\n",
      "[Training Epoch 0] Batch 534, Loss 0.11959134787321091\n",
      "[Training Epoch 0] Batch 535, Loss 0.12174651026725769\n",
      "[Training Epoch 0] Batch 536, Loss 0.1237657368183136\n",
      "[Training Epoch 0] Batch 537, Loss 0.118262380361557\n",
      "[Training Epoch 0] Batch 538, Loss 0.12089700996875763\n",
      "[Training Epoch 0] Batch 539, Loss 0.11776429414749146\n",
      "[Training Epoch 0] Batch 540, Loss 0.11721082031726837\n",
      "[Training Epoch 0] Batch 541, Loss 0.11586031317710876\n",
      "[Training Epoch 0] Batch 542, Loss 0.12203708291053772\n",
      "[Training Epoch 0] Batch 543, Loss 0.11548072844743729\n",
      "[Training Epoch 0] Batch 544, Loss 0.12213301658630371\n",
      "[Training Epoch 0] Batch 545, Loss 0.11650072038173676\n",
      "[Training Epoch 0] Batch 546, Loss 0.11647723615169525\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 61\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     58\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m sample_generator\u001b[38;5;241m.\u001b[39minstance_a_train_loader(\n\u001b[1;32m     59\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_negative\u001b[39m\u001b[38;5;124m\"\u001b[39m], config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     60\u001b[0m )\n\u001b[0;32m---> 61\u001b[0m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_an_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m hit_ratio, ndcg \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mevaluate(evaluate_data, epoch_id\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/school/jupyter_env/bt4222/ncf/src/engine.py:50\u001b[0m, in \u001b[0;36mEngine.train_an_epoch\u001b[0;34m(self, train_loader, epoch_id)\u001b[0m\n\u001b[1;32m     48\u001b[0m user, item, rating \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m], batch[\u001b[38;5;241m1\u001b[39m], batch[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     49\u001b[0m rating \u001b[38;5;241m=\u001b[39m rating\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 50\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_single_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrating\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Training Epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] Batch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Loss \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch_id, batch_id, loss)\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     54\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m~/Documents/school/jupyter_env/bt4222/ncf/src/engine.py:32\u001b[0m, in \u001b[0;36mEngine.train_single_batch\u001b[0;34m(self, users, items, ratings)\u001b[0m\n\u001b[1;32m     30\u001b[0m     users, items, ratings \u001b[38;5;241m=\u001b[39m users\u001b[38;5;241m.\u001b[39mcuda(), items\u001b[38;5;241m.\u001b[39mcuda(), ratings\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_mps\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 32\u001b[0m     users, items, ratings \u001b[38;5;241m=\u001b[39m users\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[43mitems\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, ratings\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     35\u001b[0m ratings_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(users, items)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from config import get_configs\n",
    "from gmf import GMFEngine\n",
    "from mlp import MLPEngine\n",
    "from cnn import CNNEngine\n",
    "from neumf import NeuMFEngine\n",
    "from data import SampleGenerator\n",
    "\n",
    "model = \"gmf\"\n",
    "data = \"ratings.csv\"\n",
    "\n",
    "# Load Data\n",
    "print(\"load data\")\n",
    "data = pd.read_csv(f\"./data/processed/{data}\")\n",
    "\n",
    "print(f\"Range of userId is [{data.userId.min()}, {data.userId.max()}]\")\n",
    "print(f\"Unique userIds {data.userId.nunique()}\")\n",
    "\n",
    "print(f\"Range of itemId is [{data.itemId.min()}, {data.itemId.max()}]\")\n",
    "print(f\"Unique itemIds {data.itemId.nunique()}\")\n",
    "\n",
    "print(f\"Range of rating is [{data.rating.min()}, {data.rating.max()}]\")\n",
    "print(f\"Unique ratings {data.rating.nunique()}\")\n",
    "\n",
    "num_users = data.userId.max() + 1\n",
    "num_items = data.itemId.max() + 1\n",
    "\n",
    "# DataLoader for training\n",
    "print(\"initialise sample generator\")\n",
    "sample_generator = SampleGenerator(ratings=data)\n",
    "evaluate_data = sample_generator.evaluate_data\n",
    "\n",
    "# Specify the exact model\n",
    "print(\"start traing model\")\n",
    "\n",
    "configs = get_configs(num_users, num_items)\n",
    "\n",
    "config, engine = None, None\n",
    "if model == \"gmf\":\n",
    "    config = configs[\"gmf_config\"]\n",
    "    engine = GMFEngine(config)\n",
    "elif model == \"mlp\":\n",
    "    config = configs[\"mlp_config\"]\n",
    "    engine = MLPEngine(config)\n",
    "elif model == \"cnn\":\n",
    "    config = configs[\"cnn_config\"]\n",
    "    engine = CNNEngine(config)\n",
    "elif model == \"neumf\":\n",
    "    config = configs[\"neumf_config\"]\n",
    "    engine = NeuMFEngine(config)\n",
    "\n",
    "assert config != None, \"No model chosen for training\"\n",
    "assert engine != None, \"No model chosen for training\"\n",
    "\n",
    "for epoch in range(config[\"num_epoch\"]):\n",
    "    print(\"Epoch {} starts\".format(epoch))\n",
    "    print(\"-\" * 80)\n",
    "    train_loader = sample_generator.instance_a_train_loader(\n",
    "        config[\"num_negative\"], config[\"batch_size\"]\n",
    "    )\n",
    "    engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
    "    hit_ratio, ndcg = engine.evaluate(evaluate_data, epoch_id=epoch)\n",
    "\n",
    "    if epoch == config[\"num_epoch\"] - 1:\n",
    "        engine.save(config[\"alias\"], epoch, hit_ratio, ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8516aafa-700d-4e07-ab14-7e6facd270c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
